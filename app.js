const express = require("express")

const app = express();


const data1 = [
    "1. Write a program for the following\r",
    "a.To generate an array of random numbers from a normal distribution for the array of a given shape.\r",
    "import numpy as np\r",
    "# Enter the value of n\r",
    "n=int(input('Enter no. of values:'))\r",
    "# Generates n random numbers from Normal Distribution\r",
    "rand_num = np.random.normal(0,1,n)\r",
    'print(n, " random numbers from a standard normal distribution:")\r',
    "print(rand_num)\r",
    "arr=np.array([rand_num])\r",
    "# Displays the size of the Array\r",
    "print(arr.shape)\r",
    "b. Implement Arithmetic operations on two arrays (perform broadcasting also.)\r",
    "#Generates an Array A with 0 to 11 in a 3X4 form\r",
    "A = np.arange(12).reshape(3,4)\r",
    "print(A)\r",
    "#Generates an Array B with 0 to 3 in a 1X3 form\r",
    "B = np.arange(4)\r",
    "print(B)\r",
    "#Performs the addition between A and B\r",
    "c=A+B\r",
    "print(c)\r",
    "#Similarly perform remaining arithmetic operations (Subtraction,multiplication, division )\r",
    "c. Find minimum, maximum, mean in a given array. ( in both the axes )\r",
    "arr = np.array([[11, 2, 3],[4, 5, 16],[7, 81, 22]])\r",
    "# finding the maximum and minimum element in the array\r",
    "max_element = np.max(arr)\r",
    "min_element = np.min(arr)\r",
    " \r",
    "# printing the result\r",
    "print('maximum element in the array is:', max_element)\r",
    "print('minimumm element in the array is:', min_element)\r",
    "# finding the maximum and \r",
    "# minimum element in the array\r",
    "max_element_column = np.max(arr, 0)\r",
    "max_element_row = np.max(arr, 1)\r",
    " \r",
    "min_element_column = np.amin(arr, 0)\r",
    "min_element_row = np.amin(arr, 1)\r",
    " \r",
    "# printing the result\r",
    "print('maximum elements in the columns of the array is:',max_element_column)\r",
    " \r",
    "print('maximum elements in the rows of the array is:', max_element_row)\r",
    " \r",
    "print('minimum elements in the columns of the array is:',min_element_column)\r",
    " \r",
    "print('minimum elements in the rows of the array is:',min_element_row)\r",
    "# mean of the flattened array \r",
    'print("\\nmean of arr, axis = None : ", np.mean(arr)) \r',
    " \r",
    "# mean along the axis = 0 (row-wise) \r",
    'print("\\nmean of arr, axis = 0 : ", np.mean(arr, axis = 0)) \r',
    " \r",
    "# mean along the axis = 1 (Column-wise) \r",
    'print("\\nmean of arr, axis = 1 : ", np.mean(arr, axis = 1))\r',
    "d. Implement np.arange and np.linspace functions.\r",
    "# Prints all numbers from 0 to 9 in steps of 1\r",
    "arr=np.linspace(start = 0, stop = 10, num = 11,dtype = int)\r",
    "print(arr)\r",
    "arr=np.linspace(start = 0, stop = 1, num = 11)\r",
    "print(arr)\r",
    "# Prints all numbers from 1 to 2 in steps of 0.1\r",
    "print(np.arange(1, 2, 0.1))\r",
    "e. Create a pandas series from a given list.\r",
    "# import pandas lib. as pd\r",
    "import pandas as pd\r",
    "# Assume l1 is a list of the following words\r",
    "l1 = ['ZERO', 'ONE', 'TWO', 'THREE',\r",
    "'FOUR', 'FIVE', 'SIX','SEVEN','EIGHT','NINE','TEN']\r",
    "# create Pandas Series with define indexes\r",
    "x = pd.Series(l1)\r",
    "# print the Series\r",
    "print(x)\r",
    "dtype: object\r",
    "f. Create pandas series with data and index and display the index values.\r",
    "# import pandas lib. as pd\r",
    "import pandas as pd\r",
    "# create Pandas Series with define indexes\r",
    "x = pd.Series([10, 20, 30, 40, 50], index =['a', 'b', 'c', 'd', 'e'])\r",
    "# print the Series\r",
    "print(x)\r",
    "dtype: int64\r",
    "g. Create a data frame with columns at least 5 observations\r",
    "i. select a particular column from the DataFrame\r",
    "ii. Summarize the data frame and observe the stats of the DataFrame created\r",
    "iii. Observe the mean and standard deviation of the data frame and print the values.\r",
    "import pandas as pd\r",
    "import numpy as np\r",
    "exam_data = {'name': ['Anastasia', 'Dima', 'Katherine', 'James', 'Emily'],'score': [12.5, 9, 16.5, \r",
    "np.nan, 9],'attempts': [1, 3, 2, 3, 2],\r",
    " 'qualify': ['yes', 'no', 'yes', 'no', 'no']}\r",
    "labels = ['a', 'b', 'c', 'd', 'e']\r",
    "df = pd.DataFrame(exam_data , index=labels)\r",
    'print("Dataset is as follows")\r',
    "print(df)\r",
    'print("Summary of the Dataset")\r',
    "print(df.info())\r",
    'print("Statistical values of numerical attributes")\r',
    "print(df.describe())\r",
    "meanvalue=df.score.mean()\r",
    "stdvalue=df.score.std()\r",
    "print('mean value of Score is',meanvalue)\r",
    "print('Standard deviation of score is',stdvalue)",
  ];


const data2 = [
  "2. Write a Program to determine the following in the Titanic Survival \r",
  "data.\r",
  "a. Determine the data type of each column.\r",
  "# importing all the necessary libraries\r",
  "import pandas as pd\r",
  "import numpy as np\r",
  "#we need to read the data\r",
  'data = pd.read_csv("https://raw.githubusercontent.com/naveenjoshii/Intro-to\x02MachineLearning/master/Titanic/titanic.csv")\r',
  "#print top 5 rows \r",
  "print(data.head())\r",
  "# to get the datatype of all columns we can use Dataframe.dtypes\r",
  "print(data.dtypes)\r",
  "dtype: object\r",
  "b. Find the number of non-null values in each column.\r",
  "# Dataframe.info() gives all information about every column in our dataset\r",
  "data.info()\r",
  "c. Find out the unique values in each categorical column and frequency of each unique value.\r",
  "# categorical is nothing but the datatype which is other than numerical datatype (i.e int,float \r",
  "etc).\r",
  "# to get the all categorical columns, we can use Dataframe.select_dtypes and we have to \r",
  "specify which \r",
  "#datatype we required. \r",
  '# In our case it would be "object" datatype\r',
  "categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\r",
  'print("Categorical columns are : ",categorical_cols)\r',
  'print("printing the results")\r',
  "for i in categorical_cols:\r",
  ' print("========== Column \'"+i+"\' =============")\r',
  " print(data[i].value_counts())\r",
  "d. Find the number of rows where age is greater than the mean age of data.\r",
  "# to get mean of age column\r",
  "age_mean = data['Age'].mean()\r",
  'print("Mean of Age is : ",age_mean)\r',
  'print("printing the result")\r',
  "print(np.sum(data['Age']>age_mean))\r",
  "output\r",
  "Mean of Age is : 29.69911764705882\r",
  "printing the result\r",
  "330\r",
  "e. Delete all the rows with missing values.\r",
  'print("length of dataframe before deleting rows with missing values",len(data))\r',
  "# deletes the rows where at least one element is missing\r",
  "data.dropna(inplace=True)\r",
  'print("length of dataframe after the deletion of missing value rows",len(data))\r',
  "output\r",
  "length of dataframe before deleting rows with missing values 891\r",
  "length of dataframe after the deletion of missing value rows 183",
];


const data3 = [
  "3. Perform Data Analysis on the Titanic Data Set to answer the following.\r",
  "#importing all the necessary libraries\r",
  "import pandas as pd\r",
  "import numpy as np\r",
  "import seaborn as sns\r",
  "import matplotlib.pyplot as plt\r",
  "#reading data\r",
  'data = pd.read_csv("https://raw.githubusercontent.com/naveenjoshii/Intro-to\x02MachineLearning/master/Titanic/titanic.csv")\r',
  "print(data.head()\r",
  "a. Information regarding each column of the data\r",
  "#printing the info about all the columns\r",
  "print(data.info())\r",
  "dtypes: float64(2), int64(5), object(5)\r",
  "memory usage: 83.7+ KB\r",
  "None\r",
  "b. Impact of each column on the label\r",
  "# plotting the correlation using heatmap\r",
  "sns.heatmap(data.corr(),cmap='coolwarm',xticklabels=True,annot=True)\r",
  "plt.title('data.corr()')\r",
  "c. Number of survivals in each gender\r",
  "# plotting countplot for Each gender who has survived and not survived\r",
  "sns.set_style('whitegrid')\r",
  "sns.countplot(x='Survived',hue='Sex',data=data,palette='colorblind')\r",
  "d. Number of survivals in each passenger class\r",
  "#plotting count plot for no of survivals in each class\r",
  "sns.set_style('whitegrid')\r",
  "sns.countplot(x='Survived',hue='Pclass',data=data,palette='bright')\r",
  "e. The number of people who are not alone.\r",
  "# count plot for who has siblings/spouse\r",
  "sns.countplot(x = 'SibSp', data = data,)\r",
];

const data4 = [
  "4. Perform Data Analysis on the California House Price data to answer the following\r",
  "# importing all the necessary libraries\r",
  "import pandas as pd\r",
  "import numpy as np\r",
  "#we need to read the data\r",
  'data = pd.read_csv("https://raw.githubusercontent.com/ageron/handson\x02ml/master/datasets/housing/housing.csv")\r',
  "#print top 5 rows \r",
  "print(data.head())\r",
  "a. Data Type of each column and info regarding each column\r",
  "# data information for each column\r",
  "print(data.info())\r",
  "b. The average age of a house in the data set.\r",
  "# printing average age of house\r",
  "print(data['housing_median_age'].mean())\r",
  "c. Determines top 10 localities with the high difference between income and house value. Also, top \r",
  "10 localities that have the lowest difference\r",
  "#calculating the difference btw House value and income and adding new column \r",
  "'diff_income_and_house_value' with difference values\r",
  "data['diff_income_and_house_value'] = data['median_house_value'] - data['median_income']\r",
  "# sorting the whole dataframe by the difference value in descending order\r",
  "data.sort_values(by='diff_income_and_house_value', ascending=False,inplace=True)\r",
  "#printing the top 10 localities with highest difference\r",
  'print("the top 10 localities with highest difference")\r',
  "print(data['ocean_proximity'].head(10))\r",
  "#printing the top 10 localities with lowest difference\r",
  'print("the top 10 localities with lowest difference")\r',
  "print(data['ocean_proximity'].tail(10))\r",
  "d. What is the ratio of bedrooms to total rooms in the data\r",
  "# total no of rooms\r",
  "total_rooms = data['total_rooms'].sum()\r",
  "# total number of bedrooms\r",
  "total_bedrooms = data['total_bedrooms'].sum()\r",
  "#printing the ratio of bedrooms to total rooms\r",
  "print(total_rooms//total_bedrooms)\r",
  "e. Determine the average price of a house for each type of ocean_proximity.\r",
  "# average house price for each ocean_proximity type\r",
  "data.groupby('ocean_proximity')['median_house_value'].median()",
];

const data5 = [
  "5. Write a program to perform the following tasks\r",
  "a. Determine the outliers in each non-categorical column of Titanic Data and remove them.\r",
  "# importing all the necessary libraries\r",
  "import pandas as pd\r",
  "import numpy as np\r",
  "#we need to read the data\r",
  'data = pd.read_csv("https://raw.githubusercontent.com/naveenjoshii/Intro-to\x02MachineLearning/master/Titanic/titanic.csv")\r',
  "#print top 5 rows \r",
  "print(data.head())\r",
  "# function to calculate the lower and upperbound\r",
  "def detect_outliers(data,threshold):\r",
  " mean = np.mean(data)\r",
  " std =np.std(data)\r",
  " lb = max(mean - (threshold * std),min(data))\r",
  " ub = min(mean + (threshold * std),max(data))\r",
  " return lb,ub\r",
  "df = data.copy()\r",
  'lb,ub = detect_outliers(data["Fare"],4)\r',
  "# removing the rows which are greater than upperbound\r",
  "df.drop(df[df.Fare > ub].index, inplace=True)\r",
  "# removing the rows which are less than lowerbound\r",
  "df.drop(df[df.Fare < lb ].index, inplace=True)\r",
  'lb,ub = detect_outliers(data["Age"],5)\r',
  "# removing the rows which are greater than upperbound\r",
  "df.drop(df[df.Age > ub].index, inplace=True)\r",
  "# removing the rows which are less than lowerbound\r",
  "df.drop(df[df.Age < lb].index, inplace=True)\r",
  "b. Determine missing values in each column of Titanic data. If missing values account for 30% of \r",
  "data, then remove the column.\r",
  "#printing the missing value percentage for every column\r",
  "df.isnull().mean() * 100\r",
  "dtype: float64\r",
  "# get all the column names in our dataset\r",
  "df.columns\r",
  "# As we can see cabin column has more than 30% of missing values, so we have to drop that \r",
  "column\r",
  "df.drop(['Cabin'],inplace=True,axis=1)\r",
  "# after removing the column cabin, printing the columns again. If you observe there is no \r",
  "Cabin in the output\r",
  "df.columns\r",
  "c. If missing values are less than 30% of entire data then create a new data frame\r",
  "i. Missing values in numeric columns are filled with the mean of the corresponding column.\r",
  "#printing the percentage of missing values in Age before handling\r",
  "df['Age'].isnull().mean() * 100\r",
  "# Filling the missing values with the mean of respective column\r",
  "df['Age']=df['Age'].fillna(df['Age'].mean())\r",
  "#printing the percentage of missing values in Age after handling\r",
  "df['Age'].isnull().mean() * 100\r",
  "ii. Missing values in categorical columns are filled with the most frequently occurring value.\r",
  "#printing the percentage of missing values in Embarked before handling\r",
  "df['Embarked'].isnull().mean() * 100\r",
  "# filling with filled with the most frequently occurring value.\r",
  "df[\"Embarked\"].fillna(df['Embarked'].mode()[0],inplace=True)\r",
  "#printing the percentage of missing values in Embarked after handling\r",
  "df['Embarked'].isnull().mean() * 100",
];

const data6 = [
  "6. Write a program to perform the following tasks\r",
  "a. Determine the categorical columns in Titanic Dataset. Convert Columns with string data type to \r",
  "numerical data using encoding techniques.\r",
  "#information about data\r",
  "df.info()\r",
  "print(\"each unique value and respective counts in Sex column\\n\",df['Sex'].value_counts())\r",
  "#creating another data frame for Sex column \r",
  "sex_df = pd.get_dummies(df['Sex'],drop_first=3)\r",
  "sex_df.head()\r",
  'print("each unique value and respective counts in Sex \r',
  "column\\n\",df['Embarked'].value_counts())\r",
  "# creating dummies for Embarked\r",
  "embark_df = pd.get_dummies(df['Embarked'],drop_first=True)\r",
  "embark_df.head()\r",
  "old_data = df.copy()\r",
  "# we need to drop the sex and embarked columns and replace them with the newly created \r",
  "dummies data frames\r",
  "# as Name and Tickt is not making any impact on the output label, we can drop them also\r",
  "df.drop(['Sex','PassengerId','Embarked','Name','Ticket'],axis=1,inplace=True)\r",
  "df.head()\r",
  "# After droping the Sex and Embarked columns, we are replacing them with out new data \r",
  "frames\r",
  "data = pd.concat([df,sex_df,embark_df],axis=1)\r",
  "b. Convert data in each numerical column so that it lies in the range [0,1]\r",
  "# before scaling the data\r",
  "data.head()\r",
  "# Scaling the data using minmax scaler so that values should be lies btw [0,1]\r",
  "from sklearn.preprocessing import MinMaxScaler\r",
  "scaler = MinMaxScaler()\r",
  "data[['Age','Pclass','Survived','SibSp','Parch','Fare','male','Q','S']] = \r",
  "scaler.fit_transform(data[['Age','Pclass','Survived','SibSp','Parch','Fare','male','Q','S']])\r",
  "# after scaling the data\r",
  "data.head()\r",
  "Survived Pclass Age SibSp Parch Fare male Q S\r",
  "0 0.0 1.0 0.271174 0.125 0.0 0.031865 1.0 0.0 1.0\r",
  "1 1.0 0.0 0.472229 0.125 0.0 0.313299 0.0 0.0 0.0\r",
  "2 1.0 1.0 0.321438 0.000 0.0 0.034831 0.0 0.0 1.0\r",
  "3 1.0 0.0 0.434531 0.125 0.0 0.233381 0.0 0.0 1.0\r",
  "4 0.0 1.0 0.434531 0.000 0.0 0.035381 1.0 0.0 1.0",
];

const data7 = [
  "7. Implement the following models on Titanic Dataset and determine the values of accuracy, \r",
  "precision, recall, f1 score and confusion matrix for the test data.\r",
  "data.info()\r",
  "Split the Data\r",
  "from sklearn.model_selection import train_test_split\r",
  "X_train, X_test, y_train, y_test = train_test_split(data.drop('Survived',axis=1), \r",
  " data['Survived'], test_size=0.30, \r",
  " random_state=101)\r",
  "a. Logistic Regression\r",
  "from sklearn.linear_model import LogisticRegression\r",
  "# Build the Model.\r",
  "logmodel = LogisticRegression()\r",
  "logmodel.fit(X_train,y_train)\r",
  'print("Predicting the model on the test set")\r',
  "predicted = logmodel.predict(X_test)\r",
  "Output\r",
  "Predicting the model on the test set\r",
  'print("predicted result !")\r',
  "predicted\r",
  "#confusion matrix\r",
  "from sklearn.metrics import confusion_matrix, classification_report\r",
  "print(confusion_matrix(y_test, predicted))\r",
  "# Precision Score\r",
  "from sklearn.metrics import precision_score\r",
  'print("Precision Score",precision_score(y_test,predicted))\r',
  "# Recall Score\r",
  "from sklearn.metrics import recall_score\r",
  'print("recall score",recall_score(y_test,predicted))\r',
  "# F1 Score\r",
  "from sklearn.metrics import f1_score\r",
  'print("f1 score",f1_score(y_test,predicted))\r',
  "# Classification report\r",
  "from sklearn.metrics import classification_report\r",
  "print(classification_report(y_test,predicted))\r",
  "# metrics are used to find accuracy or error\r",
  "from sklearn import metrics\r",
  "# using metrics module for accuracy calculation\r",
  'print("ACCURACY of Logistic Regression Model: ", metrics.accuracy_score(y_test, \r',
  "predicted))\r",
  "b. Random Forest Classifier\r",
  "# importing random forest classifier from assemble module\r",
  "from sklearn.ensemble import RandomForestClassifier\r",
  "# creating a RF classifier\r",
  "clf = RandomForestClassifier(n_estimators = 100)\r",
  "# Training the model on the training dataset\r",
  "# fit function is used to train the model using the training sets as parameters\r",
  "clf.fit(X_train, y_train)\r",
  "# performing predictions on the test dataset\r",
  "y_pred = clf.predict(X_test)\r",
  "#confusion matrix\r",
  "from sklearn.metrics import confusion_matrix, classification_report\r",
  "print(confusion_matrix(y_test, y_pred))\r",
  "# Precision Score\r",
  "from sklearn.metrics import precision_score\r",
  'print("Precision Score",precision_score(y_test,y_pred))\r',
  "# Recall Score\r",
  "from sklearn.metrics import recall_score\r",
  'print("recall score",recall_score(y_test,y_pred))\r',
  "# F1 Score\r",
  "from sklearn.metrics import f1_score\r",
  'print("f1 score",f1_score(y_test,y_pred))\r',
  "# Classification report\r",
  "from sklearn.metrics import classification_report\r",
  "print(classification_report(y_test,y_pred))\r",
  "# metrics are used to find accuracy or error\r",
  "from sklearn import metrics\r",
  "# using metrics module for accuracy calculation\r",
  'print("ACCURACY of Random Forest Classifier Model: ", metrics.accuracy_score(y_test, \r',
  "y_pred))",
];


const data8 = [
  "8. Implement the following models on the California House Pricing Dataset and determine the \r",
  "values of R2 score, the area under roc curve and root mean squared error for the test set.\r",
  "a. Linear Regression with Polynomial Features\r",
  "b. Random Forest Regressor\r",
  "Preparing the data\r",
  "# checking for null values\r",
  "data.isnull().mean() * 100\r",
  "# handling null values in total_bedrooms with the most frequent value in respective column\r",
  "data[\"total_bedrooms\"].fillna(data['total_bedrooms'].mode()[0],inplace=True)\r",
  "#checking the null values handled or not\r",
  'data["total_bedrooms"].isnull().mean() * 100\r',
  "data.info()\r",
  "data.info()\r",
  "data['ocean_proximity'].unique()\r",
  "#we need to convert categorical values by label encoding\r",
  "# there are more than two categories, we have to use onehot encoding\r",
  "data['ocean_proximity'].value_counts()\r",
  "ocean_prox_df = pd.get_dummies(data['ocean_proximity'],drop_first=True)\r",
  "ocean_prox_df.head()\r",
  "old_data = data.copy()\r",
  "data.drop(['ocean_proximity','longitude','latitude','diff_income_and_house_value'],axis=1,inpl\r",
  "ace=True)\r",
  "data.head()\r",
  "data = pd.concat([data,ocean_prox_df],axis=1)\r",
  "data.head()\r",
  "Split the data\r",
  "from sklearn.model_selection import train_test_split\r",
  "# split the data for training and testing\r",
  "X_train, X_test, y_train, y_test = train_test_split(data.drop('median_house_value',axis=1), \r",
  " data['median_house_value'], test_size=0.30, \r",
  "random_state=101)\r",
  "a. Linear Regression with Polynomial Features\r",
  "from sklearn.linear_model import LinearRegression\r",
  "from sklearn.preprocessing import PolynomialFeatures\r",
  "#model initialization\r",
  "model = LinearRegression()\r",
  "# initializing polynomial featuers\r",
  "poly = PolynomialFeatures(degree=3)\r",
  "#converting features into polyfeatures\r",
  "X_ = poly.fit_transform(X_train)\r",
  "Y_ = poly.fit_transform(y_train.values.reshape(-1,1))\r",
  "# training the model\r",
  "model.fit(X_,Y_)\r",
  "#preparing test data for predictions\r",
  "testX = poly.fit_transform(X_test)\r",
  "# predicting the output for test data\r",
  "predicted = model.predict(testX)\r",
  "# expected output for test data\r",
  "expected = poly.fit_transform(y_test.values.reshape(-1,1))\r",
  "from sklearn.metrics import r2_score\r",
  "r2 = r2_score(expected, predicted)\r",
  "print('r2 score is', r2)\r",
  "# example of calculate the root mean squared error\r",
  "from sklearn.metrics import mean_squared_error\r",
  "# calculate errors\r",
  "errors = mean_squared_error(expected, predicted, squared=False)\r",
  "# report error\r",
  'print("root mean square error is :",errors)\r',
  "b. Random Forest Regressor\r",
  "# Fitting Random Forest Regression to the dataset\r",
  "# import the regressor\r",
  "from sklearn.ensemble import RandomForestRegressor\r",
  " \r",
  "# create regressor object\r",
  "regressor = RandomForestRegressor(n_estimators = 100, random_state = 101)\r",
  " \r",
  "# fit the regressor with x and y data\r",
  "regressor.fit(X_train, y_train) \r",
  "# test the output by changing values\r",
  "predicted = regressor.predict(X_test)\r",
  "expected = y_test\r",
  "from sklearn.metrics import r2_score\r",
  "r2 = r2_score(expected, predicted)\r",
  "print('r2 score is', r2)\r",
  "# example of calculate the root mean squared error\r",
  "from sklearn.metrics import mean_squared_error\r",
  "# calculate errors\r",
  "errors =mean_squared_error(expected, predicted,squared=False)\r",
  "# report error\r",
  'print("root mean square error is :",errors)',
];




const data10 = [
  "10. Implement a single neural network and test for different logic gates.\r",
  "#0r gate \r",
  "import numpy as np\r",
  "def unitStep(v):\r",
  "if v >= 0:\r",
  "return 1\r",
  "else:\r",
  "return 0\r",
  "def perceptronModel(x, w, b):\r",
  "v = np.dot(w, x) + b\r",
  "y = unitStep(v)\r",
  "return y\r",
  "# OR Logic Function\r",
  "# w1 = 1, w2 = 1, b = -0.5\r",
  "def OR_logicFunction(x):\r",
  "w = np.array([1, 1])\r",
  "b = -0.5\r",
  "return perceptronModel(x, w, b)\r",
  "# testing the Perceptron Model\r",
  "test1 = np.array([0, 1])\r",
  "test2 = np.array([1, 1])\r",
  "test3 = np.array([0, 0])\r",
  "test4 = np.array([1, 0])\r",
  'print("OR({}, {}) = {}".format(0, 1, OR_logicFunction(test1)))\r',
  'print("OR({}, {}) = {}".format(1, 1, OR_logicFunction(test2)))\r',
  'print("OR({}, {}) = {}".format(0, 0, OR_logicFunction(test3)))\r',
  'print("OR({}, {}) = {}".format(1, 0, OR_logicFunction(test4)))\r',
  "# And gate\r",
  "import numpy as np\r",
  "# define Unit Step Function\r",
  "def unitStep(v):\r",
  "if v >= 0:\r",
  "return 1\r",
  "else:\r",
  "return 0\r",
  "# design Perceptron Model\r",
  "def perceptronModel(x, w, b):\r",
  "v = np.dot(w, x) + b\r",
  "y = unitStep(v)\r",
  "return y\r",
  "# AND Logic Function\r",
  "# w1 = 1, w2 = 1, b = -1.5\r",
  "def AND_logicFunction(x):\r",
  "w = np.array([1, 1])\r",
  "b = -1.5\r",
  "return perceptronModel(x, w, b)\r",
  "# testing the Perceptron Model\r",
  "test1 = np.array([0, 1])\r",
  "test2 = np.array([1, 1])\r",
  "test3 = np.array([0, 0])\r",
  "test4 = np.array([1, 0])\r",
  'print("AND({}, {}) = {}".format(0, 1, AND_logicFunction(test1)))\r',
  'print("AND({}, {}) = {}".format(1, 1, AND_logicFunction(test2)))\r',
  'print("AND({}, {}) = {}".format(0, 0, AND_logicFunction(test3)))\r',
];










// put the data in the array using the following code in python






const PORT = process.env.PORT || 5000;
app.get("/1", (req, res) => {
	res.send(data1)
})


app.get("/2", (req, res) => {
  res.send(data2);
});

app.get("/3", (req, res) => {
  res.send(data3);
});


app.get("/4", (req, res) => {
  res.send(data4);
});

app.get("/5", (req, res) => {
  res.send(data5);
});


app.get("/6", (req, res) => {
  res.send(data6);
});

app.get("/7", (req, res) => {
  res.send(data7);
});

app.get("/8", (req, res) => {
  res.send(data8);
});

app.get("/10", (req, res) => {
  res.send(data10);
});









app.listen(PORT, () => {
	console.log("started server")
})






